{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib \n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class board_state(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        b = []\n",
    "        for i in range(6):\n",
    "            b.append([0]*7)\n",
    "        self.current_state = np.array(b) \n",
    "        self.turn = 0\n",
    "        self.whos_turn = 1 \n",
    "    \n",
    "    def vect(self):\n",
    "        v = np.array([0 for i in range(6*7)])\n",
    "        for i in range(6):\n",
    "            v[7*i:7*(i+1)] = self.current_state[i] \n",
    "        return v \n",
    "        \n",
    "    def reset(self):\n",
    "        b = []\n",
    "        for i in range(6):\n",
    "            b.append([0]*7)\n",
    "        self.current_state = np.array(b) \n",
    "        self.turn =0 \n",
    "        self.whos_turn =1\n",
    "\n",
    "    def print_board(self):\n",
    "        for row in self.current_state:\n",
    "            print(''.join(row))\n",
    "\n",
    "    def play(self,n):\n",
    "        l= []\n",
    "        for i in range(6):\n",
    "            if self.current_state[i,n] ==0:\n",
    "                l.append(i)\n",
    "        if l ==[]:\n",
    "            return False\n",
    "        if self.whos_turn ==1:\n",
    "            self.current_state[max(l)][n] = 1 \n",
    "            self.whos_turn = -1 \n",
    "            self.turn +=1\n",
    "        else:\n",
    "            self.current_state[max(l)][n] = -1 \n",
    "            self.whos_turn =1\n",
    "            self.turn += 1 \n",
    "        return True\n",
    "    \n",
    "    def not_full(self):\n",
    "        nf =[]\n",
    "        for i in range(7):\n",
    "            l = []\n",
    "            for j in range(6):\n",
    "                if self.current_state[j][i] == 0:\n",
    "                    l.append(j)\n",
    "            if l!=[]:\n",
    "                nf.append(i)\n",
    "        return nf\n",
    "    \n",
    "    def check_win(self,p):\n",
    "        if p ==1:\n",
    "            s = 1 \n",
    "        else:\n",
    "            s = -1 \n",
    "\n",
    "        #check horizontal\n",
    "        def check_h(self,s):\n",
    "            for i in range(6):\n",
    "                for j in range(4):\n",
    "                    c = 0\n",
    "                    for k in range(4):\n",
    "                        if self.current_state[i][j+k]== s:\n",
    "                            c +=1\n",
    "                    if c==4:\n",
    "                        return True\n",
    "            return False\n",
    "\n",
    "        #check vertical\n",
    "        def check_v(self, s):\n",
    "            for i in range(3):\n",
    "                for j in range(7):\n",
    "                    c = 0\n",
    "                    for k in range(4):\n",
    "                        if self.current_state[i+k][j] == s:\n",
    "                            c += 1\n",
    "                        if c ==4:\n",
    "                            return True\n",
    "            return False\n",
    "\n",
    "        #check diagonal\n",
    "        def check_d(self,s):\n",
    "            for i in range(3):\n",
    "                for j in range(4):\n",
    "                    c = 0\n",
    "                    for k in range(4):\n",
    "                        if self.current_state[i+k][j+k] == s:\n",
    "                            c +=1 \n",
    "                        if c ==4 :\n",
    "                            return True\n",
    "            for i in range(3):\n",
    "                for j in range(3,7):\n",
    "                    c = 0\n",
    "                    for k in range(4):\n",
    "                        if self.current_state[i+k][j-k] == s:\n",
    "                            c +=1\n",
    "                        if c ==4:\n",
    "                            return True\n",
    "            return False\n",
    "            #combind   \n",
    "        if check_h(self,s) or check_v(self,s) or check_d(self,s):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def step(self,n):\n",
    "        played = self.play(n)\n",
    "        end = False\n",
    "        if self.check_win(1) or self.check_win(2) or self.turn==42:\n",
    "                end = True\n",
    "        return [self.vect(), played, end]\n",
    "        \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 256)\n",
    "        self.layer3 = nn.Linear(256,128)\n",
    "        self.layer4 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        return self.layer4(x)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = 7 \n",
    "# Get the number of state observations\n",
    "board = board_state()\n",
    "board.reset()\n",
    "state = board.vect() \n",
    "n_observations = len(state) \n",
    "\n",
    "policy_net_1 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_1 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_1.load_state_dict(policy_net_1.state_dict())\n",
    "\n",
    "optimizer_1 = optim.AdamW(policy_net_1.parameters(), lr=LR, amsgrad=True)\n",
    "memory_1 = ReplayMemory(10000)\n",
    "\n",
    "policy_net_2 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_2 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_2.load_state_dict(policy_net_2.state_dict())\n",
    "\n",
    "optimizer_2 = optim.AdamW(policy_net_2.parameters(), lr=LR, amsgrad=True)\n",
    "memory_2 = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "def select_action(state,p, learn=True):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold =  EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if not learn:\n",
    "        eps_threshold = 0\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            if p==1:\n",
    "                action_values = policy_net_1(state)[0,:]\n",
    "                #    return policy_net_1(state).max(1).indices.view(1,1) \n",
    "            elif p==-1:\n",
    "                action_values = policy_net_2(state)[0,:]\n",
    "            available_action_values = [action_values[i] for i in board.not_full()]\n",
    "            return torch.tensor([[np.argmax(available_action_values)]], device=device, dtype=torch.long)\n",
    "\n",
    "            #    return policy_net_2(state).max(1).indices.view(1,1)\n",
    "    else:\n",
    "        r = random.randint(0,6)\n",
    "        return torch.tensor([[r]], device=device, dtype=torch.long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model_1():\n",
    "    if len(memory_1) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory_1.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net_1(state_batch).gather(1, action_batch )\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device) \n",
    "    #with torch.no_grad():\n",
    "    next_state_values[non_final_mask] = target_net_1(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    #criterion = nn.SmoothL1Loss()\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer_1.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net_1.parameters(), 100)\n",
    "    optimizer_1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model_2():\n",
    "    if len(memory_2) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory_2.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net_2(state_batch).gather(1, action_batch )\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device) \n",
    "    #with torch.no_grad():\n",
    "    next_state_values[non_final_mask] = target_net_2(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    #criterion = nn.SmoothL1Loss()\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "    optimizer_2.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net_2.parameters(), 100)\n",
    "    optimizer_2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04617844149470329\n",
      "0.055209219455718994\n",
      "0.0404340885579586\n",
      "0.033072877675294876\n",
      "0.03692561388015747\n",
      "0.024013904854655266\n",
      "0.019757598638534546\n",
      "0.02468668855726719\n",
      "0.03697308897972107\n",
      "0.0321788415312767\n",
      "0.03664565458893776\n",
      "0.04216886684298515\n",
      "0.024277254939079285\n",
      "0.0325053445994854\n",
      "0.02002270705997944\n",
      "0.02342190407216549\n",
      "0.036151785403490067\n",
      "0.041824132204055786\n",
      "0.06245682016015053\n",
      "0.07247016578912735\n",
      "0.052203934639692307\n",
      "0.050232477486133575\n",
      "0.030202031135559082\n",
      "0.04273911193013191\n",
      "0.03981446102261543\n",
      "0.03831805661320686\n",
      "0.0346391424536705\n",
      "0.08021149784326553\n",
      "0.042972005903720856\n",
      "0.03424577787518501\n",
      "0.046980082988739014\n",
      "0.04423099756240845\n",
      "0.0406961627304554\n",
      "0.04343470185995102\n",
      "0.04589097574353218\n",
      "0.11757436394691467\n",
      "0.10580890625715256\n",
      "0.059419695287942886\n",
      "0.08666791021823883\n",
      "0.04333816096186638\n",
      "0.07863745093345642\n",
      "0.10801386833190918\n",
      "0.1098170280456543\n",
      "0.08737368881702423\n",
      "0.057898227125406265\n",
      "0.06138019636273384\n",
      "0.0620943121612072\n",
      "0.08789293467998505\n",
      "0.06815486401319504\n",
      "0.05146172270178795\n",
      "0.13666726648807526\n",
      "0.10876324772834778\n",
      "0.11218005418777466\n",
      "0.06346394866704941\n",
      "0.10691369324922562\n",
      "0.06892392039299011\n",
      "0.14031429588794708\n",
      "0.17444391548633575\n",
      "0.07424955815076828\n",
      "0.05875676870346069\n",
      "0.0640706717967987\n",
      "0.06814739108085632\n",
      "0.15863613784313202\n",
      "0.13435447216033936\n",
      "0.09332825988531113\n",
      "0.11770077049732208\n",
      "0.15069960057735443\n",
      "0.11246694624423981\n",
      "0.10639515519142151\n",
      "0.07992128282785416\n",
      "0.20975545048713684\n",
      "0.10172414779663086\n",
      "0.09594088047742844\n",
      "0.08099699020385742\n",
      "0.07235519587993622\n",
      "0.1035756915807724\n",
      "0.08069503307342529\n",
      "0.07806170731782913\n",
      "0.10856413841247559\n",
      "0.10473087430000305\n",
      "0.17398734390735626\n",
      "0.16043151915073395\n",
      "0.18574652075767517\n",
      "0.20411589741706848\n",
      "0.10215017199516296\n",
      "0.12157132476568222\n",
      "0.12947675585746765\n",
      "0.09898597002029419\n",
      "0.1595880687236786\n",
      "0.1587287336587906\n",
      "0.11590473353862762\n",
      "0.16714678704738617\n",
      "0.10818503051996231\n",
      "0.2415485978126526\n",
      "0.11572587490081787\n",
      "0.17146170139312744\n",
      "0.15667587518692017\n",
      "0.2008894383907318\n",
      "0.13039319217205048\n",
      "0.13407361507415771\n",
      "0.15674953162670135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[279], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m         state_1 \u001b[39m=\u001b[39m next_state_1\n\u001b[1;32m     66\u001b[0m         \u001b[39m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m optimize_model_1()\n\u001b[1;32m     68\u001b[0m optimize_model_2()\n\u001b[1;32m     69\u001b[0m \u001b[39m# Soft update of the target network's weights\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[277], line 42\u001b[0m, in \u001b[0;36moptimize_model_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m# Optimize the model\u001b[39;00m\n\u001b[1;32m     41\u001b[0m optimizer_1\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 42\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     43\u001b[0m \u001b[39m# In-place gradient clipping\u001b[39;00m\n\u001b[1;32m     44\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_value_(policy_net_1\u001b[39m.\u001b[39mparameters(), \u001b[39m100\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 500 \n",
    "copy_episodes =1 \n",
    "win1=0\n",
    "win2=0\n",
    "draw=0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    board.reset()\n",
    "    state_1= board.vect()\n",
    "    state_2= board.vect()\n",
    "    state_1 = torch.tensor(state_1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    state_2 = torch.tensor(state_2, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        # Player 1\n",
    "        played = False\n",
    "        while not played:\n",
    "            action_1  = select_action(state_1,board.whos_turn)\n",
    "            observation, played, terminated = board.step(action_1.item())\n",
    "       \n",
    "        if terminated:\n",
    "            next_state_1 = None\n",
    "            next_state_2 = None\n",
    "            reward_1 = torch.tensor([1], device=device) \n",
    "            reward_2 = torch.tensor([-1], device= device)\n",
    "            memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "            memory_2.push(state_2,action_2,next_state_2, reward_2)\n",
    "            win1 +=1\n",
    "     \n",
    "        else:\n",
    "            reward_2 = torch.tensor([0], device=device)\n",
    "            next_state_2 = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            if board.turn>1:\n",
    "                memory_2.push(state_2,action_2,next_state_2, reward_2)\n",
    "            # Player 2\n",
    "            state_2 = next_state_2\n",
    "            played = False\n",
    "            while not played:\n",
    "                action_2 = select_action(state_2, board.whos_turn)\n",
    "                observation, played, terminated = board.step(action_2.item())\n",
    "            if terminated and board.turn !=42:\n",
    "                next_state_1 = None\n",
    "                next_state_2 = None\n",
    "                reward_1 = torch.tensor([-1], device=device)\n",
    "                reward_2 = torch.tensor([1],device=device) \n",
    "                win2 +=1\n",
    "                memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "                memory_2.push(state_2, action_2, next_state_2, reward_2)\n",
    "               \n",
    "            elif terminated and board.turn ==42:\n",
    "                next_state_1 = None\n",
    "                next_state_2 = None\n",
    "                reward_1 = torch.tensor([0.5], device = device)\n",
    "                reward_2 = torch.tensor([0.5], device = device)\n",
    "                memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "                memory_2.push(state_2, action_2, next_state_2, reward_2)\n",
    "                draw +=1\n",
    "            else:\n",
    "                next_state_1 = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                reward_1 = torch.tensor([0],device=device)\n",
    "            \n",
    "                # Store the transition in memory and move to next state\n",
    "                memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "                state_1 = next_state_1\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model_1()\n",
    "        optimize_model_2()\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        if i_episode % copy_episodes ==0:\n",
    "            #update model 1\n",
    "            target_net_1_state_dict = target_net_1.state_dict()\n",
    "            policy_net_1_state_dict = policy_net_1.state_dict()\n",
    "            for key in policy_net_1_state_dict:\n",
    "                target_net_1_state_dict[key] = policy_net_1_state_dict[key]*TAU + target_net_1_state_dict[key]*(1-TAU)\n",
    "            target_net_1.load_state_dict(target_net_1_state_dict)\n",
    "            #update model 2\n",
    "            target_net_2_state_dict = target_net_2.state_dict()\n",
    "            policy_net_2_state_dict = policy_net_2.state_dict()\n",
    "            for key in policy_net_2_state_dict:\n",
    "                target_net_2_state_dict[key] = policy_net_2_state_dict[key]*TAU + target_net_2_state_dict[key]*(1-TAU)\n",
    "            target_net_2.load_state_dict(target_net_2_state_dict)\n",
    "\n",
    "        if terminated: \n",
    "           break\n",
    "    \n",
    "\n",
    "print('Complete')\n",
    "print(\"Player 1 won \" +str(win1) +\" times.\")\n",
    "print(\"Player 2 won \" +str(win2) +\" times.\")\n",
    "print(\"There were \" +str(draw) + \" draws.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net_1.state_dict(), \"trained_1.pth\")\n",
    "torch.save(policy_net_2.state_dict(), \"trained_2.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab935af0a0e958ea0b52b2713abb138d4cc7261c752c5e375a54ec983817d59b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
