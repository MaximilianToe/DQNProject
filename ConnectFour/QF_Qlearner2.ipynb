{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib \n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class board_state(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        b = []\n",
    "        for i in range(6):\n",
    "            b.append([0]*7)\n",
    "        self.current_state = np.array(b) \n",
    "        self.turn = 0\n",
    "        self.whos_turn = 1 \n",
    "    \n",
    "    #returns the current state of the board as a vector\n",
    "    def vect(self):\n",
    "        v = np.array([0 for i in range(6*7)])\n",
    "        for i in range(6):\n",
    "            v[7*i:7*(i+1)] = self.current_state[i] \n",
    "        return v \n",
    "\n",
    "    #resets the board to play a new game    \n",
    "    def reset(self):\n",
    "        b = []\n",
    "        for i in range(6):\n",
    "            b.append([0]*7)\n",
    "        self.current_state = np.array(b) \n",
    "        self.turn =0 \n",
    "        self.whos_turn =1\n",
    "\n",
    "    def print_board(self):\n",
    "        for row in self.current_state:\n",
    "            print(''.join(row))\n",
    "\n",
    "    #takes a number from 0 to 6 and places to chip in the next free spot in that column (if that column is not already full)\n",
    "    def play(self,n):\n",
    "        l= []\n",
    "        for i in range(6):\n",
    "            if self.current_state[i,n] ==0:\n",
    "                l.append(i)\n",
    "        if l ==[]:\n",
    "            return False\n",
    "        if self.whos_turn ==1:\n",
    "            self.current_state[max(l)][n] = 1 \n",
    "            self.whos_turn = -1 \n",
    "            self.turn +=1\n",
    "        else:\n",
    "            self.current_state[max(l)][n] = -1 \n",
    "            self.whos_turn =1\n",
    "            self.turn += 1 \n",
    "        return True\n",
    "\n",
    "   #returns a list that contains the indices of all columns that are not full \n",
    "    def not_full(self):\n",
    "        not_full =[]\n",
    "        for i in range(7):\n",
    "            l = []\n",
    "            for j in range(6):\n",
    "                if self.current_state[j][i] == 0:\n",
    "                    l.append(j)\n",
    "            if l!=[]:\n",
    "                not_full.append(i)\n",
    "        return not_full \n",
    "\n",
    "    #checks whether there are four chips in a row in any of the admissible directions. p determines for which player the condition is checked.\n",
    "    def check_win(self,p):\n",
    "        if p ==1:\n",
    "            s = 1 \n",
    "        else:\n",
    "            s = -1 \n",
    "\n",
    "        #check horizontal\n",
    "        def check_h(self,s):\n",
    "            for i in range(6):\n",
    "                for j in range(4):\n",
    "                    c = 0\n",
    "                    for k in range(4):\n",
    "                        if self.current_state[i][j+k]== s:\n",
    "                            c +=1\n",
    "                    if c==4:\n",
    "                        return True\n",
    "            return False\n",
    "\n",
    "        #check vertical\n",
    "        def check_v(self, s):\n",
    "            for i in range(3):\n",
    "                for j in range(7):\n",
    "                    c = 0\n",
    "                    for k in range(4):\n",
    "                        if self.current_state[i+k][j] == s:\n",
    "                            c += 1\n",
    "                        if c ==4:\n",
    "                            return True\n",
    "            return False\n",
    "\n",
    "        #check diagonal\n",
    "        def check_d(self,s):\n",
    "            for i in range(3):\n",
    "                for j in range(4):\n",
    "                    c = 0\n",
    "                    for k in range(4):\n",
    "                        if self.current_state[i+k][j+k] == s:\n",
    "                            c +=1 \n",
    "                        if c ==4 :\n",
    "                            return True\n",
    "            for i in range(3):\n",
    "                for j in range(3,7):\n",
    "                    c = 0\n",
    "                    for k in range(4):\n",
    "                        if self.current_state[i+k][j-k] == s:\n",
    "                            c +=1\n",
    "                        if c ==4:\n",
    "                            return True\n",
    "            return False\n",
    "            #combind   \n",
    "        if check_h(self,s) or check_v(self,s) or check_d(self,s):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    #used to perform a single step during the learning proceedure. Positions a chip in the n-th column and returns the resulting state as a vector, whether the move was valid and whether the game ended.\n",
    "    def step(self,n):\n",
    "        played = self.play(n)\n",
    "        end = False\n",
    "        if self.check_win(1) or self.check_win(2) or self.turn==42:\n",
    "                end = True\n",
    "        return [self.vect(), played, end]\n",
    "        \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines a class that is used to store the data we obtain during learning\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "#defines a class of DQN of fixed depth and matrix size\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 256)\n",
    "        self.layer3 = nn.Linear(256,128)\n",
    "        self.layer4 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        return self.layer4(x)\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 1 #0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#initializes the enviroment \n",
    "board = board_state()\n",
    "state = board.vect() \n",
    "n_actions = 7\n",
    "n_observations = len(state) \n",
    "\n",
    "##initialize neural networks. policy_net will be optimized in each step and target_net will be soft updated from policy_net by the specified rate TAU\n",
    "\n",
    "#initializes the network, optimizer and memory for player 1\n",
    "policy_net_1 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_1 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_1.load_state_dict(policy_net_1.state_dict())\n",
    "\n",
    "optimizer_1 = optim.AdamW(policy_net_1.parameters(), lr=LR, amsgrad=True)\n",
    "memory_1 = ReplayMemory(10000)\n",
    "\n",
    "#initializes the network, optimizer and memory for player 2\n",
    "policy_net_2 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_2 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_2.load_state_dict(policy_net_2.state_dict())\n",
    "\n",
    "optimizer_2 = optim.AdamW(policy_net_2.parameters(), lr=LR, amsgrad=True)\n",
    "memory_2 = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "def select_action(state,p, learn=True):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold =  EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if not learn:\n",
    "        eps_threshold = 0\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            #evaluates the network to obtain a vector for ALL moves\n",
    "            if p==1:\n",
    "                action_values = policy_net_1(state)[0,:] \n",
    "            elif p==-1:\n",
    "                action_values = policy_net_2(state)[0,:]\n",
    "            #selects the playable moves and returns the maximum as a tensor \n",
    "            available_action_values = [action_values[i] for i in board.not_full()]\n",
    "            return torch.tensor([[np.argmax(available_action_values)]], device=device, dtype=torch.long) \n",
    "    else:\n",
    "        r = random.randint(0,6)\n",
    "        return torch.tensor([[r]], device=device, dtype=torch.long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimizes the temporal difference error for player 1\n",
    "def optimize_model_1():\n",
    "    if len(memory_1) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory_1.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net_1(state_batch).gather(1, action_batch )\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device) \n",
    "    #with torch.no_grad():\n",
    "    next_state_values[non_final_mask] = target_net_1(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    #criterion = nn.SmoothL1Loss()\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer_1.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net_1.parameters(), 100)\n",
    "    optimizer_1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimizers the temporal difference error for player 2\n",
    "\n",
    "def optimize_model_2():\n",
    "    if len(memory_2) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory_2.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net_2(state_batch).gather(1, action_batch )\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device) \n",
    "    #with torch.no_grad():\n",
    "    next_state_values[non_final_mask] = target_net_2(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    #criterion = nn.SmoothL1Loss()\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "    optimizer_2.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net_2.parameters(), 100)\n",
    "    optimizer_2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "Complete\n",
      "Player 1 won 933 times.\n",
      "Player 2 won 1049 times.\n",
      "There were 18 draws.\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 2000 \n",
    "win1=0\n",
    "win2=0\n",
    "draw=0\n",
    "\n",
    "#each i_episode corresponds to a single game of connect four while each t in count() corresponds to a single turn.\n",
    "#There are no rewards unless the game ends. In that case a win yields +1, a defeat -1, and a draw 0.5.\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    board.reset()\n",
    "    state_1= board.vect()\n",
    "    state_2= board.vect()\n",
    "    state_1 = torch.tensor(state_1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    state_2 = torch.tensor(state_2, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        # Player 1\n",
    "        played = False\n",
    "        while not played:\n",
    "            action_1  = select_action(state_1,board.whos_turn)\n",
    "            observation, played, terminated = board.step(action_1.item())\n",
    "       \n",
    "        if terminated:\n",
    "            next_state_1 = None\n",
    "            next_state_2 = None\n",
    "            reward_1 = torch.tensor([1], device=device) \n",
    "            reward_2 = torch.tensor([-1], device= device)\n",
    "            memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "            memory_2.push(state_2,action_2,next_state_2, reward_2)\n",
    "            win1 +=1\n",
    "     \n",
    "        else:\n",
    "            reward_2 = torch.tensor([0], device=device)\n",
    "            next_state_2 = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            if board.turn>1:\n",
    "                memory_2.push(state_2,action_2,next_state_2, reward_2)\n",
    "            # Player 2\n",
    "            state_2 = next_state_2\n",
    "            played = False\n",
    "            while not played:\n",
    "                action_2 = select_action(state_2, board.whos_turn)\n",
    "                observation, played, terminated = board.step(action_2.item())\n",
    "            if terminated and board.turn !=42:\n",
    "                next_state_1 = None\n",
    "                next_state_2 = None\n",
    "                reward_1 = torch.tensor([-1], device=device)\n",
    "                reward_2 = torch.tensor([1],device=device) \n",
    "                win2 +=1\n",
    "                memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "                memory_2.push(state_2, action_2, next_state_2, reward_2)\n",
    "               \n",
    "            elif terminated and board.turn ==42:\n",
    "                next_state_1 = None\n",
    "                next_state_2 = None\n",
    "                reward_1 = torch.tensor([0.5], device = device)\n",
    "                reward_2 = torch.tensor([0.5], device = device)\n",
    "                memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "                memory_2.push(state_2, action_2, next_state_2, reward_2)\n",
    "                draw +=1\n",
    "            else:\n",
    "                next_state_1 = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                reward_1 = torch.tensor([0],device=device)\n",
    "            \n",
    "                # Store the transition in memory and move to next state\n",
    "                memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "                state_1 = next_state_1\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model_1()\n",
    "        optimize_model_2()\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        #update model 1\n",
    "        target_net_1_state_dict = target_net_1.state_dict()\n",
    "        policy_net_1_state_dict = policy_net_1.state_dict()\n",
    "        for key in policy_net_1_state_dict:\n",
    "            target_net_1_state_dict[key] = policy_net_1_state_dict[key]*TAU + target_net_1_state_dict[key]*(1-TAU)\n",
    "        target_net_1.load_state_dict(target_net_1_state_dict)\n",
    "        #update model 2\n",
    "        target_net_2_state_dict = target_net_2.state_dict()\n",
    "        policy_net_2_state_dict = policy_net_2.state_dict()\n",
    "        for key in policy_net_2_state_dict:\n",
    "            target_net_2_state_dict[key] = policy_net_2_state_dict[key]*TAU + target_net_2_state_dict[key]*(1-TAU)\n",
    "        target_net_2.load_state_dict(target_net_2_state_dict)\n",
    "        if terminated: \n",
    "           break\n",
    "    if i_episode%50==0:\n",
    "        print(i_episode)\n",
    "    \n",
    "\n",
    "print('Complete')\n",
    "print(\"Player 1 won \" +str(win1) +\" times.\")\n",
    "print(\"Player 2 won \" +str(win2) +\" times.\")\n",
    "print(\"There were \" +str(draw) + \" draws.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net_1.state_dict(), \"trained_1.pth\")\n",
    "torch.save(policy_net_2.state_dict(), \"trained_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a class that represents a player without knowledge, i.e. randomly selects a (possible) action\n",
    "\n",
    "class randomPlayer():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def play(self,board):\n",
    "        action = random.choice(board.not_full())\n",
    "        return board.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines a class the represents a player that chooses moves based on the trained model\n",
    "\n",
    "class trainedPlayer():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def play(self,board):\n",
    "        state = torch.tensor(board.vect(), dtype=torch.float).unsqueeze(0)\n",
    "        if board.whos_turn==1:\n",
    "            action = select_action(state, 1, False).item()\n",
    "        if board.whos_turn==-1:\n",
    "            action = select_action(state,-1, False).item()\n",
    "        return board.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724\n",
      "276\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Checks performence of the trained player against a random player\n",
    "\n",
    "winsPlayer1= 0\n",
    "winsPlayer2= 0\n",
    "draws = 0\n",
    "Player1 = trainedPlayer()\n",
    "Player2 = randomPlayer()\n",
    "\n",
    "for i in range(1000):\n",
    "    board = board_state()\n",
    "    for t in count():\n",
    "        done = Player1.play(board)[2]\n",
    "        if done:\n",
    "            winsPlayer1 +=1\n",
    "            break\n",
    "        done = Player2.play(board)[2]\n",
    "        if done:\n",
    "            if board.check_win(2):\n",
    "                winsPlayer2 +=1\n",
    "                break\n",
    "            else:\n",
    "                draws +=1\n",
    "                break\n",
    "\n",
    "print(winsPlayer1)\n",
    "print(winsPlayer2)\n",
    "print(draws)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab935af0a0e958ea0b52b2713abb138d4cc7261c752c5e375a54ec983817d59b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
