{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#import Connect Four from other file\n",
    "from CFGameLogic import ConnectFour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines a class that is used to store the data we obtain during learning\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "#defines a class of DQN of fixed depth and matrix size\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 256)\n",
    "        self.layer3 = nn.Linear(256,128)\n",
    "        self.layer4 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        return self.layer4(x)\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024 #128\n",
    "GAMMA =  0.90\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.001\n",
    "LR = 1e-6\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#initializes the environment \n",
    "board = ConnectFour()\n",
    "state = board.vect() \n",
    "n_actions = 7\n",
    "n_observations = len(state) \n",
    "\n",
    "##initialize neural networks. policy_net will be optimized in each step and target_net will be soft updated from policy_net by the specified rate TAU\n",
    "\n",
    "#initializes the network, optimizer and memory for player 1\n",
    "policy_net_1 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_1 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_1.load_state_dict(policy_net_1.state_dict())\n",
    "\n",
    "optimizer_1 = optim.AdamW(policy_net_1.parameters(), lr=LR, amsgrad=True)\n",
    "memory_1 = ReplayMemory(50000)\n",
    "\n",
    "#initializes the network, optimizer and memory for player 2\n",
    "policy_net_2 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_2 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_2.load_state_dict(policy_net_2.state_dict())\n",
    "\n",
    "optimizer_2 = optim.AdamW(policy_net_2.parameters(), lr=LR, amsgrad=True)\n",
    "memory_2 = ReplayMemory(50000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "def select_action(state,p, learn=True):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold =  EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if not learn:\n",
    "        eps_threshold = 0\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            #evaluates the network to obtain a vector for ALL moves\n",
    "            if p==1:\n",
    "                action_values = policy_net_1(state) \n",
    "            elif p==-1:\n",
    "                action_values = policy_net_2(state)\n",
    "            #selects the playable moves and returns the maximum as a tensor \n",
    "            max_index_nf =  torch.argmax(action_values[0,board.not_full()])\n",
    "            return torch.tensor([[board.not_full()[max_index_nf]]], device=device, dtype=torch.long)\n",
    "            #available_action_values = [action_values[i] for i in board.not_full()]\n",
    "            #m = np.max(available_action_values)\n",
    "            #return (action_values ==m).nonzero(as_tuple=False)\n",
    "            #return torch.tensor([[np.argmax(available_action_values)]], device=device, dtype=torch.long) \n",
    "    else:\n",
    "        r = random.choice(board.not_full())\n",
    "        return torch.tensor([[r]], device=device, dtype=torch.long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimizes the temporal difference error for player 1\n",
    "def optimize_model_1():\n",
    "    if len(memory_1) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory_1.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net_1(state_batch).gather(1, action_batch )\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device) \n",
    "    #with torch.no_grad():\n",
    "    next_state_values[non_final_mask] = target_net_1(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    #criterion = nn.SmoothL1Loss()\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer_1.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net_1.parameters(), 100)\n",
    "    optimizer_1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimizers the temporal difference error for player 2\n",
    "\n",
    "def optimize_model_2():\n",
    "    if len(memory_2) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory_2.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net_2(state_batch).gather(1, action_batch )\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device) \n",
    "    #with torch.no_grad():\n",
    "    next_state_values[non_final_mask] = target_net_2(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    #criterion = nn.SmoothL1Loss()\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "\n",
    "    optimizer_2.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net_2.parameters(), 100)\n",
    "    optimizer_2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 episodes done.\n",
      "100 episodes done.\n",
      "200 episodes done.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m played \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m played:\n\u001b[0;32m---> 19\u001b[0m     action_1  \u001b[39m=\u001b[39m select_action(state_1,board\u001b[39m.\u001b[39;49mwhos_turn)\n\u001b[1;32m     20\u001b[0m     observation, played, terminated \u001b[39m=\u001b[39m board\u001b[39m.\u001b[39mstep(action_1\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m terminated:\n",
      "Cell \u001b[0;32mIn[18], line 91\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(state, p, learn)\u001b[0m\n\u001b[1;32m     89\u001b[0m             action_values \u001b[39m=\u001b[39m policy_net_2(state)\n\u001b[1;32m     90\u001b[0m         \u001b[39m#selects the playable moves and returns the maximum as a tensor \u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m         max_index_nf \u001b[39m=\u001b[39m  torch\u001b[39m.\u001b[39;49margmax(action_values[\u001b[39m0\u001b[39;49m,board\u001b[39m.\u001b[39;49mnot_full()])\n\u001b[1;32m     92\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor([[board\u001b[39m.\u001b[39mnot_full()[max_index_nf]]], device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m     93\u001b[0m         \u001b[39m#available_action_values = [action_values[i] for i in board.not_full()]\u001b[39;00m\n\u001b[1;32m     94\u001b[0m         \u001b[39m#m = np.max(available_action_values)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m         \u001b[39m#return (action_values ==m).nonzero(as_tuple=False)\u001b[39;00m\n\u001b[1;32m     96\u001b[0m         \u001b[39m#return torch.tensor([[np.argmax(available_action_values)]], device=device, dtype=torch.long) \u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 5000 \n",
    "win1=0\n",
    "win2=0\n",
    "draw=0\n",
    "\n",
    "#each i_episode corresponds to a single game of connect four while each t in count() corresponds to a single turn.\n",
    "#There are no rewards unless the game ends. In that case a win yields +1, a defeat -1, and a draw 0.5.\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    board.reset()\n",
    "    state_1= board.vect()\n",
    "    state_2= board.vect()\n",
    "    state_1 = torch.tensor(state_1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    state_2 = torch.tensor(state_2, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        # Player 1\n",
    "        played = False\n",
    "        while not played:\n",
    "            action_1  = select_action(state_1,board.whos_turn)\n",
    "            observation, played, terminated = board.step(action_1.item())\n",
    "       \n",
    "        if terminated:\n",
    "            next_state_1 = None\n",
    "            next_state_2 = None\n",
    "            reward_1 = torch.tensor([1], device=device) \n",
    "            reward_2 = torch.tensor([-1], device= device)\n",
    "            memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "            memory_2.push(state_2,action_2,next_state_2, reward_2)\n",
    "            win1 +=1\n",
    "     \n",
    "        else:\n",
    "            reward_2 = torch.tensor([0], device=device)\n",
    "            next_state_2 = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            if board.turn>1:\n",
    "                memory_2.push(state_2,action_2,next_state_2, reward_2)\n",
    "            # Player 2\n",
    "            state_2 = next_state_2\n",
    "            played = False\n",
    "            while not played:\n",
    "                action_2 = select_action(state_2, board.whos_turn)\n",
    "                observation, played, terminated = board.step(action_2.item())\n",
    "            if terminated and board.turn !=42:\n",
    "                next_state_1 = None\n",
    "                next_state_2 = None\n",
    "                reward_1 = torch.tensor([-1], device=device)\n",
    "                reward_2 = torch.tensor([1],device=device) \n",
    "                win2 +=1\n",
    "                memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "                memory_2.push(state_2, action_2, next_state_2, reward_2)\n",
    "               \n",
    "            elif terminated and board.turn ==42:\n",
    "                next_state_1 = None\n",
    "                next_state_2 = None\n",
    "                reward_1 = torch.tensor([0.5], device = device)\n",
    "                reward_2 = torch.tensor([0.5], device = device)\n",
    "                memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "                memory_2.push(state_2, action_2, next_state_2, reward_2)\n",
    "                draw +=1\n",
    "            else:\n",
    "                next_state_1 = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                reward_1 = torch.tensor([0],device=device)\n",
    "            \n",
    "                # Store the transition in memory and move to next state\n",
    "                memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "                state_1 = next_state_1\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model_1()\n",
    "        optimize_model_2()\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        #update model 1\n",
    "        target_net_1_state_dict = target_net_1.state_dict()\n",
    "        policy_net_1_state_dict = policy_net_1.state_dict()\n",
    "        for key in policy_net_1_state_dict:\n",
    "            target_net_1_state_dict[key] = policy_net_1_state_dict[key]*TAU + target_net_1_state_dict[key]*(1-TAU)\n",
    "        target_net_1.load_state_dict(target_net_1_state_dict)\n",
    "        #update model 2\n",
    "        target_net_2_state_dict = target_net_2.state_dict()\n",
    "        policy_net_2_state_dict = policy_net_2.state_dict()\n",
    "        for key in policy_net_2_state_dict:\n",
    "            target_net_2_state_dict[key] = policy_net_2_state_dict[key]*TAU + target_net_2_state_dict[key]*(1-TAU)\n",
    "        target_net_2.load_state_dict(target_net_2_state_dict)\n",
    "        if terminated: \n",
    "           break\n",
    "    if i_episode%100==0 and i_episode >0:\n",
    "        print(str(i_episode)+\" episodes done.\")\n",
    "    \n",
    "\n",
    "print('Complete')\n",
    "print(\"Player 1 won \" +str(win1) +\" times.\")\n",
    "print(\"Player 2 won \" +str(win2) +\" times.\")\n",
    "print(\"There were \" +str(draw) + \" draws.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net_1.state_dict(), \"trained_1.pth\")\n",
    "torch.save(policy_net_2.state_dict(), \"trained_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net_1 = DQN(n_observations, n_actions).to(device)\n",
    "policy_net_2 = DQN(n_observations, n_actions).to(device)\n",
    "policy_net_1.load_state_dict(torch.load(\"trained_1_5000eps.pth\"))\n",
    "policy_net_2.load_state_dict(torch.load(\"trained_2_5000eps.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines a class the represents a player that chooses moves based on the trained model\n",
    "class trainedPlayer():\n",
    "\n",
    "    def __init__(self, ran):\n",
    "        self.random = ran\n",
    "\n",
    "    def play(self,board):\n",
    "        state = torch.tensor(board.vect(), dtype=torch.float).unsqueeze(0)\n",
    "        if board.whos_turn==1:\n",
    "            action = select_action(state, 1, self.random).item()\n",
    "        if board.whos_turn==-1:\n",
    "            action = select_action(state,-1, self.random).item()\n",
    "        return board.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "776\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Checks performence of the trained player against a random player\n",
    "\n",
    "from CFPlayer import randomPlayer, alphaBetaPlayer\n",
    "\n",
    "winsPlayer1= 0\n",
    "winsPlayer2= 0\n",
    "draws = 0\n",
    "Player2 = trainedPlayer(True) \n",
    "Player1 = randomPlayer() \n",
    "\n",
    "for i in range(1000):\n",
    "    board = ConnectFour()\n",
    "    for t in count():\n",
    "        done = Player1.play(board)[2]\n",
    "\n",
    "        if done:\n",
    "            winsPlayer1 +=1\n",
    "            break\n",
    "        done = Player2.play(board)[2]\n",
    "\n",
    "        if done:\n",
    "            if board.check_win(2):\n",
    "                winsPlayer2 +=1\n",
    "                break\n",
    "            else:\n",
    "                draws +=1\n",
    "                break\n",
    "\n",
    "print(winsPlayer1)\n",
    "print(winsPlayer2)\n",
    "print(draws)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m     winsPlayer1 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     17\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m done \u001b[39m=\u001b[39m Player2\u001b[39m.\u001b[39;49mplay(board)[\u001b[39m2\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m     21\u001b[0m     \u001b[39mif\u001b[39;00m board\u001b[39m.\u001b[39mcheck_win(\u001b[39m2\u001b[39m):\n",
      "File \u001b[0;32m~/testDQNProjects/ConnectFour/CFPlayer.py:22\u001b[0m, in \u001b[0;36malphaBetaPlayer.play\u001b[0;34m(self, board)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplay\u001b[39m(\u001b[39mself\u001b[39m,board):\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m board\u001b[39m.\u001b[39mstep(alphaBeta(board, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeepth))\n",
      "File \u001b[0;32m~/testDQNProjects/ConnectFour/CFalphaBeta.py:65\u001b[0m, in \u001b[0;36malphaBeta\u001b[0;34m(board, depth)\u001b[0m\n\u001b[1;32m     63\u001b[0m new_board \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(board)\n\u001b[1;32m     64\u001b[0m new_board\u001b[39m.\u001b[39mplay(action)\n\u001b[0;32m---> 65\u001b[0m next_value \u001b[39m=\u001b[39m  max_player(new_board,depth \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49mmath\u001b[39m.\u001b[39;49minf, math\u001b[39m.\u001b[39;49minf)\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m next_value\u001b[39m==\u001b[39mv:\n\u001b[1;32m     67\u001b[0m     \u001b[39m#if action has the same value we also save it\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     best_action\u001b[39m.\u001b[39mappend(action)\n",
      "File \u001b[0;32m~/testDQNProjects/ConnectFour/CFalphaBeta.py:6\u001b[0m, in \u001b[0;36mmax_player\u001b[0;34m(board, depth, alpha, beta)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmax_player\u001b[39m(board,depth,  alpha , beta):\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mif\u001b[39;00m board\u001b[39m.\u001b[39;49mcheck_win(\u001b[39m2\u001b[39;49m):\n\u001b[1;32m      7\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      8\u001b[0m     \u001b[39melif\u001b[39;00m board\u001b[39m.\u001b[39mturn\u001b[39m==\u001b[39m\u001b[39m42\u001b[39m \u001b[39mor\u001b[39;00m depth\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/testDQNProjects/ConnectFour/CFGameLogic.py:115\u001b[0m, in \u001b[0;36mConnectFour.check_win\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[39m#combind   \u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m \u001b[39mif\u001b[39;00m check_h(\u001b[39mself\u001b[39;49m,s) \u001b[39mor\u001b[39;00m check_v(\u001b[39mself\u001b[39m,s) \u001b[39mor\u001b[39;00m check_d(\u001b[39mself\u001b[39m,s):\n\u001b[1;32m    116\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/testDQNProjects/ConnectFour/CFGameLogic.py:76\u001b[0m, in \u001b[0;36mConnectFour.check_win.<locals>.check_h\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m):\n\u001b[1;32m     75\u001b[0m     c \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(\u001b[39m4\u001b[39;49m):\n\u001b[1;32m     77\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_state[i][j\u001b[39m+\u001b[39mk]\u001b[39m==\u001b[39m s:\n\u001b[1;32m     78\u001b[0m             c \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Player1 = trainedPlayer(ran =True)\n",
    "#note that we have to allow player 1 to sometimes choose a random action. Otherwise every game would play out the same.\n",
    "number_of_games = 200\n",
    "max_depth =4\n",
    "number_of_wins_Player1 =[]\n",
    "for d in range(1,max_depth+1):\n",
    "    #initializes a player with alpha-beta pruning of depth d\n",
    "    Player2 = alphaBetaPlayer(d)\n",
    "    winsPlayer1 =0 \n",
    "    for i in range(number_of_games):\n",
    "        board = ConnectFour()\n",
    "        for t in count():\n",
    "            done = Player1.play(board)[2]\n",
    "\n",
    "            if done:\n",
    "                winsPlayer1 +=1\n",
    "                break\n",
    "            done = Player2.play(board)[2]\n",
    "\n",
    "            if done:\n",
    "                if board.check_win(2):\n",
    "                    break\n",
    "                else:\n",
    "                    break   \n",
    "    number_of_wins_Player1.append(winsPlayer1)\n",
    "\n",
    "number_of_wins_Player1 = [element/number_of_games *100 for element in winsPlayer1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.bar([\"1\",\"2\",\"3\",\"4\"], number_of_wins_Player1, edgecolor='black')\n",
    "\n",
    "plt.xlabel('search depth')\n",
    "plt.ylabel('win% player 1')\n",
    "plt.title('win% for each search depth after 5000 episodes')\n",
    "#plt.savefig('winsDepth.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab935af0a0e958ea0b52b2713abb138d4cc7261c752c5e375a54ec983817d59b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
