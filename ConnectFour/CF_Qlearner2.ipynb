{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#import Connect Four from other file\n",
    "from CFGameLogic import ConnectFour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines a class that is used to store the data we obtain during learning\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "#defines a class of DQN of fixed depth and matrix size\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 256)\n",
    "        self.layer3 = nn.Linear(256,128)\n",
    "        self.layer4 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        return self.layer4(x)\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1024 #128\n",
    "GAMMA =  0.90\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.001\n",
    "LR = 1e-6\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#initializes the environment \n",
    "board = ConnectFour()\n",
    "state = board.vect() \n",
    "n_actions = 7\n",
    "n_observations = len(state) \n",
    "\n",
    "##initialize neural networks. policy_net will be optimized in each step and target_net will be soft updated from policy_net by the specified rate TAU\n",
    "\n",
    "#initializes the network, optimizer and memory for player 1\n",
    "policy_net_1 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_1 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_1.load_state_dict(policy_net_1.state_dict())\n",
    "\n",
    "optimizer_1 = optim.AdamW(policy_net_1.parameters(), lr=LR, amsgrad=True)\n",
    "memory_1 = ReplayMemory(50000)\n",
    "\n",
    "#initializes the network, optimizer and memory for player 2\n",
    "policy_net_2 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_2 = DQN(n_observations, n_actions).to(device)\n",
    "target_net_2.load_state_dict(policy_net_2.state_dict())\n",
    "\n",
    "optimizer_2 = optim.AdamW(policy_net_2.parameters(), lr=LR, amsgrad=True)\n",
    "memory_2 = ReplayMemory(50000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "def select_action(state,p, learn=True):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold =  EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if not learn:\n",
    "        eps_threshold = 0\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            #evaluates the network to obtain a vector for ALL moves\n",
    "            if p==1:\n",
    "                action_values = policy_net_1(state) \n",
    "            elif p==-1:\n",
    "                action_values = policy_net_2(state)\n",
    "            #selects the playable moves and returns the maximum as a tensor \n",
    "            max_index_nf =  torch.argmax(action_values[0,board.not_full()])\n",
    "            return torch.tensor([[board.not_full()[max_index_nf]]], device=device, dtype=torch.long)\n",
    "            #available_action_values = [action_values[i] for i in board.not_full()]\n",
    "            #m = np.max(available_action_values)\n",
    "            #return (action_values ==m).nonzero(as_tuple=False)\n",
    "            #return torch.tensor([[np.argmax(available_action_values)]], device=device, dtype=torch.long) \n",
    "    else:\n",
    "        r = random.choice(board.not_full())\n",
    "        return torch.tensor([[r]], device=device, dtype=torch.long)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimizes the temporal difference error for player 1\n",
    "def optimize_model_1():\n",
    "    if len(memory_1) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory_1.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net_1(state_batch).gather(1, action_batch )\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device) \n",
    "    #with torch.no_grad():\n",
    "    next_state_values[non_final_mask] = target_net_1(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    #criterion = nn.SmoothL1Loss()\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer_1.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net_1.parameters(), 100)\n",
    "    optimizer_1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimizers the temporal difference error for player 2\n",
    "\n",
    "def optimize_model_2():\n",
    "    if len(memory_2) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory_2.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net_2(state_batch).gather(1, action_batch )\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device) \n",
    "    #with torch.no_grad():\n",
    "    next_state_values[non_final_mask] = target_net_2(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    #criterion = nn.SmoothL1Loss()\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "\n",
    "    optimizer_2.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net_2.parameters(), 100)\n",
    "    optimizer_2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5000 \n",
    "win1=0\n",
    "win2=0\n",
    "draw=0\n",
    "\n",
    "#each i_episode corresponds to a single game of connect four while each t in count() corresponds to a single turn.\n",
    "#There are no rewards unless the game ends. In that case a win yields +1, a defeat -1, and a draw 0.5.\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    board.reset()\n",
    "    state_1= board.vect()\n",
    "    state_2= board.vect()\n",
    "    state_1 = torch.tensor(state_1, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    state_2 = torch.tensor(state_2, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        # Player 1\n",
    "        played = False\n",
    "        while not played:\n",
    "            action_1  = select_action(state_1,board.whos_turn)\n",
    "            observation, played, terminated = board.step(action_1.item())\n",
    "       \n",
    "        if terminated:\n",
    "            next_state_1 = None\n",
    "            next_state_2 = None\n",
    "            reward_1 = torch.tensor([1], device=device) \n",
    "            reward_2 = torch.tensor([-1], device= device)\n",
    "            memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "            memory_2.push(state_2,action_2,next_state_2, reward_2)\n",
    "            win1 +=1\n",
    "     \n",
    "        else:\n",
    "            reward_2 = torch.tensor([0], device=device)\n",
    "            next_state_2 = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            if board.turn>1:\n",
    "                memory_2.push(state_2,action_2,next_state_2, reward_2)\n",
    "            # Player 2\n",
    "            state_2 = next_state_2\n",
    "            played = False\n",
    "            while not played:\n",
    "                action_2 = select_action(state_2, board.whos_turn)\n",
    "                observation, played, terminated = board.step(action_2.item())\n",
    "            if terminated and board.turn !=42:\n",
    "                next_state_1 = None\n",
    "                next_state_2 = None\n",
    "                reward_1 = torch.tensor([-1], device=device)\n",
    "                reward_2 = torch.tensor([1],device=device) \n",
    "                win2 +=1\n",
    "                memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "                memory_2.push(state_2, action_2, next_state_2, reward_2)\n",
    "               \n",
    "            elif terminated and board.turn ==42:\n",
    "                next_state_1 = None\n",
    "                next_state_2 = None\n",
    "                reward_1 = torch.tensor([0.5], device = device)\n",
    "                reward_2 = torch.tensor([0.5], device = device)\n",
    "                memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "                memory_2.push(state_2, action_2, next_state_2, reward_2)\n",
    "                draw +=1\n",
    "            else:\n",
    "                next_state_1 = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                reward_1 = torch.tensor([0],device=device)\n",
    "            \n",
    "                # Store the transition in memory and move to next state\n",
    "                memory_1.push(state_1, action_1, next_state_1, reward_1)\n",
    "                state_1 = next_state_1\n",
    "\n",
    "                # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model_1()\n",
    "        optimize_model_2()\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        #update model 1\n",
    "        target_net_1_state_dict = target_net_1.state_dict()\n",
    "        policy_net_1_state_dict = policy_net_1.state_dict()\n",
    "        for key in policy_net_1_state_dict:\n",
    "            target_net_1_state_dict[key] = policy_net_1_state_dict[key]*TAU + target_net_1_state_dict[key]*(1-TAU)\n",
    "        target_net_1.load_state_dict(target_net_1_state_dict)\n",
    "        #update model 2\n",
    "        target_net_2_state_dict = target_net_2.state_dict()\n",
    "        policy_net_2_state_dict = policy_net_2.state_dict()\n",
    "        for key in policy_net_2_state_dict:\n",
    "            target_net_2_state_dict[key] = policy_net_2_state_dict[key]*TAU + target_net_2_state_dict[key]*(1-TAU)\n",
    "        target_net_2.load_state_dict(target_net_2_state_dict)\n",
    "        if terminated: \n",
    "           break\n",
    "    if i_episode%100==0 and i_episode >0:\n",
    "        print(str(i_episode)+\" episodes done.\")\n",
    "    \n",
    "\n",
    "print('Complete')\n",
    "print(\"Player 1 won \" +str(win1) +\" times.\")\n",
    "print(\"Player 2 won \" +str(win2) +\" times.\")\n",
    "print(\"There were \" +str(draw) + \" draws.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net_1.state_dict(), \"trained_1.pth\")\n",
    "torch.save(policy_net_2.state_dict(), \"trained_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net_1 = DQN(n_observations, n_actions).to(device)\n",
    "policy_net_2 = DQN(n_observations, n_actions).to(device)\n",
    "policy_net_1.load_state_dict(torch.load(\"trained_1_5000eps.pth\"))\n",
    "policy_net_2.load_state_dict(torch.load(\"trained_2_5000eps.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defines a class the represents a player that chooses moves based on the trained model\n",
    "class trainedPlayer():\n",
    "\n",
    "    def __init__(self, ran):\n",
    "        self.random = ran\n",
    "\n",
    "    def play(self,board):\n",
    "        state = torch.tensor(board.vect(), dtype=torch.float, device=device).unsqueeze(0)\n",
    "        if board.whos_turn==1:\n",
    "            action = select_action(state, 1, self.random).item()\n",
    "        if board.whos_turn==-1:\n",
    "            action = select_action(state,-1, self.random).item()\n",
    "        return board.step(action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks performence of the trained player against a random player\n",
    "\n",
    "from CFPlayer import randomPlayer, alphaBetaPlayer\n",
    "\n",
    "winsPlayer1= 0\n",
    "winsPlayer2= 0\n",
    "draws = 0\n",
    "Player2 = trainedPlayer(True) \n",
    "Player1 = randomPlayer() \n",
    "\n",
    "for i in range(1000):\n",
    "    board = ConnectFour()\n",
    "    for t in count():\n",
    "        done = Player1.play(board)[2]\n",
    "\n",
    "        if done:\n",
    "            winsPlayer1 +=1\n",
    "            break\n",
    "        done = Player2.play(board)[2]\n",
    "\n",
    "        if done:\n",
    "            if board.check_win(2):\n",
    "                winsPlayer2 +=1\n",
    "                break\n",
    "            else:\n",
    "                draws +=1\n",
    "                break\n",
    "\n",
    "print(winsPlayer1)\n",
    "print(winsPlayer2)\n",
    "print(draws)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Player1 = trainedPlayer(ran =True)\n",
    "#note that we have to allow player 1 to sometimes choose a random action. Otherwise every game would play out the same.\n",
    "number_of_games = 200\n",
    "max_depth =4\n",
    "number_of_wins_Player1 =[]\n",
    "for d in range(1,max_depth+1):\n",
    "    #initializes a player with alpha-beta pruning of depth d\n",
    "    Player2 = alphaBetaPlayer(d)\n",
    "    winsPlayer1 =0 \n",
    "    for i in range(number_of_games):\n",
    "        board = ConnectFour()\n",
    "        for t in count():\n",
    "            done = Player1.play(board)[2]\n",
    "\n",
    "            if done:\n",
    "                winsPlayer1 +=1\n",
    "                break\n",
    "            done = Player2.play(board)[2]\n",
    "\n",
    "            if done:\n",
    "                if board.check_win(2):\n",
    "                    break\n",
    "                else:\n",
    "                    break   \n",
    "    number_of_wins_Player1.append(winsPlayer1)\n",
    "\n",
    "number_of_wins_Player1 = [element/number_of_games *100 for element in winsPlayer1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.bar([\"1\",\"2\",\"3\",\"4\"], number_of_wins_Player1, edgecolor='black')\n",
    "\n",
    "plt.xlabel('search depth')\n",
    "plt.ylabel('win% player 1')\n",
    "plt.title('win% for each search depth after 5000 episodes')\n",
    "#plt.savefig('winsDepth.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab935af0a0e958ea0b52b2713abb138d4cc7261c752c5e375a54ec983817d59b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
